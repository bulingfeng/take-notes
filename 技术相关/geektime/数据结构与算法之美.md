## 复杂度分析

### 为什么需要复杂度分析

> 有人说可以通过让代码跑一遍，然后进行统计和监控来得到结果。然后跑多次来进行结果的对比。这种方法叫做**事后统计法**。
>
> 这种方法有很严重的缺陷：
>
> > 1. 测试结果非常依赖于环境；
> > 2. 测试结果非常受数据规模的影响；

### 大O复杂度表示法

> 不运行代码的情况下，一眼能看出来代码的执行效率。

方法可以把代码运行的每一行代码的时间都认为是时间一样的，暂且当做unit_time。那么以下代码执行了多长时间呢：

```java
 int cal(int n) {
   int sum = 0;
   int i = 1;
   for (; i <= n; ++i) {
     sum = sum + i;
   }
   return sum;
 }
```

最后的时间是(2n+2)*unit_time。

```java
 int cal(int n) {
   int sum = 0;
   int i = 1;
   int j = 1;
   for (; i <= n; ++i) {
     j = 1;
     for (; j <= n; ++j) {
       sum = sum +  i * j;
     }
   }
 }
```

以上的代码的执行时间为：T(n) = (2n2+2n+3)*unit_time。

> 而不管是(2n+2)*unit_time还是(2n2+2n+3)*unit_time。我们都可以把其描述为f(n)的一个函数。
>
> 即大O表示O(f(n));更具体的解释为：O(2n+2)和O(2n2+2n+3)。
>
> 通常只需要考虑增长最快的因素即可，常量、系数已经变化较小的公式都可以舍去，于是上面的大O表示法可以表示为：T(n) = O(n)； T(n) = O(n2)

需要注意的是，这个大O表示法并不直接代表代码运行的时间，**它只是代表随着数据规模的变化而代码执行消耗时间的趋势**，叫做渐进时间复杂度也叫时间负责度。

快速判断时间复杂度的小技巧：

> 只关注运行代码随着数据规模增长最快的部分即可。

常见的时间复杂度案例分析

![](./image/数据结构与算法之美/1-常见的时间复杂度.png)

上面的复杂度可以分为两种：多项式量级和非多项式量级。

> 当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。

### O(1)

O(1)并不是只执行了一行代码，只是常量级的时间复杂度表示方法。如下代码：

```java
 int i = 8;
 int j = 6;
 int sum = i + j;
```

> 只要代码的执行时间不随 n 的增大而增长，这样代码的时间复杂度我们都记作 O(1)。或者说，一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。

### O(logn)、O(nlogn)

```java
 i=1;
 while (i <= n)  {
   i = i * 2;
 }
```

其实我们就是这个循环里面的代码执行了几次：x=log2n；

```java
 i=1;
 while (i <= n)  {
   i = i * 3;
 }
// 这段代码的时间复杂度为 O(log3n)。
```

不管是以 2 为底、以 3 为底，还是以 10 为底，我们可以把所有对数阶的时间复杂度都记为 O(logn)。

如果一段代码的时间复杂度是 O(logn)，我们循环执行 n 遍，时间复杂度就是 O(nlogn) 了;

### O(m+n)、O(m*n)

```java
int cal(int m, int n) {
  int sum_1 = 0;
  int i = 1;
  for (; i < m; ++i) {
    sum_1 = sum_1 + i;
  }

  int sum_2 = 0;
  int j = 1;
  for (; j < n; ++j) {
    sum_2 = sum_2 + j;
  }

  return sum_1 + sum_2;
}
```

>m 和 n 是表示两个数据规模。我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是 O(m+n)。
>
>针对这种情况，原来的加法法则就不正确了，我们需要将加法规则改为：T1(m) + T2(n) = O(f(m) + g(n));
>
>但是乘法法则继续有效：T1(m)*T2(n) = O(f(m) * f(n))。

![](./image/数据结构与算法之美/2-时间复杂度趋势变化图.png)

## 更详细的时间复杂度分析

> - 最好情况时间复杂度
> - 最坏情况时间复杂度
> - 平均情况时间复杂度
> - 均摊时间复杂度

## 数组：为什么很多编程语言的数组都是从0开始的

什么是数组？

> 数组（Array）是一种**线性表数据结构**。它用一组**连续的**内存空间，来存储一组具有**相同类型**的数据;

> a[k]_address = base_address + k * type_size
>
> a[k]_address = base_address + (k-1)*type_size
>
> 从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于 CPU 来说，就是多了一次减法指令。
>
> 其中的k代表的是首地址。比如a[0]代表的首字母是0

## 链表

链表的分类

> 1. 单向链表；
> 2. 单向循环链表；
> 3. 双向链表；

![](./image/数据结构与算法之美/3-单向链表.png)

![](./image/数据结构与算法之美/4-单链表插入和删除.png)

![](./image/数据结构与算法之美/5-循环链表.png)

![](./image/数据结构与算法之美/6-双向链表.png)

一些误区

> 1. 既然链表的**删除**是O(1),那么单向链表和双向链表的删除效率是一样的？因为还需要进行查询，所以查询的时间是O(n),但是如果制定某个节点，那么双向链表查询就是O(1),因为它天然存储了上一个节点指针。
> 2. 链表的删除主要的时间其实都浪费在查询上了。

链表和数组的区别

> 链表是不需要连续内存的；而数组是需要连续内存的；
>
> 链表添加和删除数据消耗较小，而数组添加和删除开销严重；

LRU的实现方式

> 1. 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。2. 如果此数据没有在缓存链表中，又可以分为两种情况：如果此时缓存未满，则将此结点直接插入到链表的头部；如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。这样我们就用链表实现了一个 LRU 缓存，是不是很简单？

## Hash算法

> 任意长度的二进制字符串映射为固定长度的二进制字符串，这个映射规则就叫做hash算法。

Hash算法一般满足以下要求

> - hash值不能反向推导出来原始数据
> - 对数据敏感，哪怕修改任何一个bit的数据都会导致结果大不一样
> - 散列的冲突要小，对于不同的原始数据，hash值的冲突率要小
> - hash算法的执行效率要高，针对长文本也能够快速得到hash值

### Hash算法应用

#### 安全加密

常用于加密的算法有MD5,SHA,DES,AES

> hash算法有两个重要的特性：
>
> 1. 无法通过hash值来反推原始值
> 2. 对于不同的值冲突尽量的少
>
> 而对于`不同的值冲突尽量的少`，其实是从数学上来解释的。因为hash的值是固定的，而输出的值可以是无限的，所以理论上来说肯定是会有不同的值冲突的。只不过概率可能很小。
>
> 比如前面举的 MD5 的例子，哈希值是固定的 128 位二进制串，能表示的数据是有限的，最多能表示 2^128 个数据，而我们要哈希的数据是无穷的。基于鸽巢原理，如果我们对 2^128+1 个数据求哈希值，就必然会存在哈希值相同的情况。

#### 唯一标识

>  判断视频和图片是否存在过,可以对视频或者图片做hash从而来判断这个文件是存在过。

#### 数据校验

> 比如电驴上下来一个2G的电影，但是需要分成100个块。于是这个种子上有这100个文件的hash值，当100个文件下载好之后，再进行hash，然后对比种子种的hash值是否一样，如果不一样就认为此文件被修改过，需要再次从服务器上下载。

#### 散列函数

比如HashMap的实现。而这种方式其实不关心值是否会重复，散列更关注的是是否能够均匀的分配到各个槽之中。

> 对于密码的“脱库”，其实仅仅使用SHA等加密算法还是不够的，因为有些用户的密码实在太简单，比如“0000”，“123456”之类的，如果黑客有个常用的字典表然后去强烈破解，那么就会得到相对于的密码（虽然从理论上来讲，这个破解的密码不一定是用户的密码，应为hash是有几率冲突的）
>
> 所以还需要引入一个salt来增加密码的安全性。

